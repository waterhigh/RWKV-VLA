# builder.py

import torch
import torch.nn as nn
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
import os
import argparse # ç”¨äºä¸ºæµ‹è¯•åˆ›å»ºé…ç½®å¯¹è±¡

# ====================================================================================
# å…³é”®ä¾èµ–é¡¹å¯¼å…¥
# ç¡®ä¿è¿™äº›æ–‡ä»¶ç›¸å¯¹äºæ‚¨çš„é¡¹ç›®æ ¹ç›®å½•æ˜¯å¯è®¿é—®çš„
# å¦‚æœè¿è¡Œæ—¶å‡ºç° ModuleNotFoundErrorï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´è¿™äº›å¯¼å…¥è·¯å¾„
# ä¾‹å¦‚ï¼Œå»æ‰å‰é¢çš„ '.' (from .hidden_model -> from hidden_model)
# ====================================================================================
from src.hidden_model import VisualRWKV
from policy_heads.models.droid_unet_diffusion import ConditionalUnet1D, SinusoidalPosEmb, Downsample1d, Upsample1d, Conv1dBlock, ConditionalResidualBlock1D
from src.rwkv_tokenizer import TRIE_TOKENIZER
from src.dataset import process_image_tokens_in_conversations, preprocess
from src.utils import Conversation

# å®šä¹‰ä¸€äº›VLMå¯èƒ½éœ€è¦çš„å¸¸é‡
IGNORE_INDEX = -100
DEFAULT_IMAGE_TOKEN = "<image>"

class RWKVDroidPolicy(nn.Module):
    """
    é›†æˆçš„ç­–ç•¥æ¨¡å‹ï¼Œå†…éƒ¨åŒ…å«äº†å®Œæ•´çš„VLMè¾“å…¥é¢„å¤„ç†é€»è¾‘ï¼Œå¹¶ä½¿ç”¨çœŸå®ç»„ä»¶ã€‚
    """
    def __init__(self,
                 # æ–‡ä»¶è·¯å¾„
                 vlm_model_path: str,
                 tokenizer_path: str,
                 # VLM é…ç½®
                 vlm_config_args,
                 # Diffusion Head & Action é…ç½®
                 action_dim: int,
                 state_dim: int,
                 action_horizon: int,
                 down_dims: list = [256, 512, 1024],
                 kernel_size: int = 5,
                 n_groups: int = 8,
                 # Scheduler é…ç½®
                 num_train_timesteps: int = 100,
                 num_inference_steps: int = 10,
                 beta_schedule: str = 'squaredcos_cap_v2',
                 prediction_type: str = 'epsilon'
                ):
        super().__init__()
        
        # ä¿å­˜åŠ¨ä½œ/çŠ¶æ€é…ç½®
        self.action_dim = action_dim
        self.action_horizon = action_horizon
        self.num_inference_steps = num_inference_steps
        
        # 1. åˆå§‹åŒ– VLM å’Œ Tokenizer (ä½¿ç”¨çœŸå®ç»„ä»¶å’Œè·¯å¾„)
        print("æ­£åœ¨åˆå§‹åŒ– VisualRWKV...")
        self.vlm = VisualRWKV(vlm_config_args)
        print(f"æ­£åœ¨ä» {vlm_model_path} åŠ è½½VLMæƒé‡...")
        self.vlm.load_state_dict(torch.load(vlm_model_path, map_location='cpu'), strict=False)
        self.vlm.eval().bfloat16() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å’Œbf16
        
        print(f"æ­£åœ¨ä» {tokenizer_path} åˆå§‹åŒ– TRIE_TOKENIZER...")
        self.tokenizer = TRIE_TOKENIZER(tokenizer_path)
        
        # 2. å†»ç»“VLM
        print("æ­£åœ¨å†»ç»“ VisualRWKV çš„å‚æ•°...")
        for param in self.vlm.parameters():
            param.requires_grad = False
            
        self.global_cond_dim = vlm_config_args.n_embd
        print(f"VLM å…¨å±€æ¡ä»¶ç»´åº¦å·²ç¡®è®¤ä¸º: {self.global_cond_dim}")

        # 3. åˆå§‹åŒ– Diffusion Head
        print("æ­£åœ¨åˆå§‹åŒ– ConditionalUnet1D Diffusion Head...")
        self.diffusion_head = ConditionalUnet1D(
            input_dim=self.action_dim,
            global_cond_dim=self.global_cond_dim,
            state_dim=state_dim,
            down_dims=down_dims,
            kernel_size=kernel_size,
            n_groups=n_groups
        )
        
        # 4. åˆå§‹åŒ–å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDIMScheduler(
            num_train_timesteps=num_train_timesteps,
            beta_schedule=beta_schedule,
            clip_sample=True,
            set_alpha_to_one=True,
            steps_offset=0,
            prediction_type=prediction_type
        )
        
        # ä¿å­˜é¢„å¤„ç†éœ€è¦çš„å‚æ•°
        self.ctx_len = vlm_config_args.ctx_len
        self.image_position = getattr(vlm_config_args, 'image_position', 'first')

    def _prepare_vlm_input(self, image_input, text_instructions, device):
        """ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œå°è£…äº†ä»demo.pyå­¦åˆ°çš„å®Œæ•´é¢„å¤„ç†é€»è¾‘ã€‚"""
        batch_size = image_input.shape[0]
        all_input_ids = []
        all_labels = []

        for i in range(batch_size):
            instruction = text_instructions[i]
            input_text = DEFAULT_IMAGE_TOKEN + ' ' + instruction

            conv = Conversation(id=f"session_{i}", roles=["human", "gpt"], conversations=[])
            conv.append_message(conv.roles[0], input_text)
            
            conversations_processed = process_image_tokens_in_conversations(
                conv.conversations, image_position=self.image_position
            )
            
            data_dict = preprocess(
                conversations_processed,
                self.tokenizer,
                has_image=True,
                ctx_len=self.ctx_len,
                pad_token_id=0,
                do_pad_to_max_length=False
            )
            all_input_ids.append(data_dict['input_ids'])
            all_labels.append(data_dict['labels'])
        
        max_len = max(len(ids) for ids in all_input_ids)
        
        padded_input_ids = torch.zeros(batch_size, max_len, dtype=torch.long, device=device)
        padded_labels = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=torch.long, device=device)
        
        for i in range(batch_size):
            len_ids = len(all_input_ids[i])
            padded_input_ids[i, :len_ids] = all_input_ids[i]
            padded_labels[i, :len_ids] = all_labels[i]

        samples = {
            "images": image_input.bfloat16(),
            "input_ids": padded_input_ids,
            "labels": padded_labels
        }
        return samples

    def forward(self, image_input, text_instructions, robot_state, ground_truth_actions):
        """è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­"""
        device = image_input.device
        
        with torch.no_grad():
             samples = self._prepare_vlm_input(image_input, text_instructions, device)
             global_condition = self.vlm.get_global_condition(samples)
        
        noise = torch.randn_like(ground_truth_actions)
        timesteps = torch.randint(
            0, self.noise_scheduler.config.num_train_timesteps, 
            (ground_truth_actions.shape[0],), device=device
        ).long()
        noisy_actions = self.noise_scheduler.add_noise(ground_truth_actions, noise, timesteps)
        
        predicted_noise = self.diffusion_head(
            sample=noisy_actions,
            timestep=timesteps,
            global_cond=global_condition,
            states=robot_state
        )
        
        return predicted_noise, noise

# ====================================================================================
#                           ä¸»æµ‹è¯•ç”¨ä¾‹ (ä½¿ç”¨çœŸå®ç»„ä»¶)
# ====================================================================================

if __name__ == '__main__':
    print("="*60)
    print("RUNNING RWKVDroidPolicy REAL INTEGRATION TEST")
    print("="*60)

    # ##################################################################
    # ### ç”¨æˆ·ï¼šè¯·åœ¨æ­¤å¤„ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„                       ###
    # ##################################################################
    VLM_MODEL_PATH = "/home/bgi/code/VLA/weights/VisualRWKV-v060-1B6-v1.0-20240612.pth"
    TOKENIZER_PATH = "src/rwkv_vocab_v20230424.txt"
    # ##################################################################

    # --- 1. è®¾ç½®æµ‹è¯•å‚æ•° ---
    ACTION_DIM = 7  # ä¾‹å¦‚: 6-DoFæœ«ç«¯æ‰§è¡Œå™¨ + 1å¤¹çˆªçŠ¶æ€
    STATE_DIM = 7   # ä¾‹å¦‚: æœºå™¨äººè‡ªèº«çš„æœ¬ä½“æ„Ÿå—çŠ¶æ€
    ACTION_HORIZON = 16
    BATCH_SIZE = 2  # ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"Test Configuration:")
    print(f"  - Device: {DEVICE}")
    print(f"  - Batch Size: {BATCH_SIZE}")
    print(f"  - Action Dim: {ACTION_DIM}")
    print(f"  - State Dim: {STATE_DIM}")
    print(f"  - VLM Model: {VLM_MODEL_PATH}")
    print(f"  - Tokenizer: {TOKENIZER_PATH}")

    # --- 2. åˆ›å»ºVLMæ‰€éœ€çš„é…ç½®å¯¹è±¡ (æ¨¡ä»¿æ‚¨çš„demo.py) ---
    vlm_config_args = argparse.Namespace(
        n_layer=24,
        n_embd=2048,
        ctx_len=256, # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        vocab_size=65536,
        dim_att=2048,
        dim_ffn=7168,
        pre_ffn=0,
        head_size_a=64,
        head_size_divisor=8,
        dropout=0.0,
        grad_cp=0,
        grid_size=-1,
        image_position='first'
    )
    os.environ["RWKV_HEAD_SIZE_A"] = str(vlm_config_args.head_size_a)

    # --- 3. å®ä¾‹åŒ–æˆ‘ä»¬çš„ä¸»ç­–ç•¥æ¨¡å‹ ---
    try:
        policy = RWKVDroidPolicy(
            vlm_model_path=VLM_MODEL_PATH,
            tokenizer_path=TOKENIZER_PATH,
            vlm_config_args=vlm_config_args,
            action_dim=ACTION_DIM,
            state_dim=STATE_DIM,
            action_horizon=ACTION_HORIZON
        ).to(DEVICE)
    except FileNotFoundError as e:
        print(f"[ERROR] æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿ä¸Šé¢çš„ VLM_MODEL_PATH å’Œ TOKENIZER_PATH æ˜¯æ­£ç¡®çš„ã€‚")
        exit()
    except Exception as e:
        print(f"[ERROR] åˆå§‹åŒ– RWKVDroidPolicy æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        exit()
        
    print("--- RWKVDroidPolicy Initialized Successfully ---")

    # --- 4. åˆ›å»ºéšæœºè¾“å…¥æ•°æ® ---
    dummy_image_input = torch.randn(BATCH_SIZE, 3, 224, 224, device=DEVICE)
    dummy_text_instructions = [f"pick up the red block" for _ in range(BATCH_SIZE)]
    dummy_robot_state = torch.randn(BATCH_SIZE, STATE_DIM, device=DEVICE, dtype=torch.bfloat16)
    dummy_ground_truth_actions = torch.randn(BATCH_SIZE, ACTION_HORIZON, ACTION_DIM, device=DEVICE, dtype=torch.bfloat16)

    print("--- Testing Training Forward Pass ---")
    print(f"Input action shape: {dummy_ground_truth_actions.shape}")
    
    # --- 5. è¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ ---
    try:
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ (å°½ç®¡VLMæ˜¯å†»ç»“çš„ï¼Œä½†diffusion headéœ€è¦)
        policy.train()
        
        predicted_noise, original_noise = policy.forward(
            image_input=dummy_image_input,
            text_instructions=dummy_text_instructions,
            robot_state=dummy_robot_state,
            ground_truth_actions=dummy_ground_truth_actions
        )
        
        print("--- FORWARD PASS COMPLETED ---")
        print(f"Output predicted_noise shape: {predicted_noise.shape}")
        
        # --- 6. éªŒè¯è¾“å‡ºå½¢çŠ¶ ---
        assert predicted_noise.shape == dummy_ground_truth_actions.shape
        print("[SUCCESS] Output shape matches input action shape.")
        
        print("" + "="*60)
        print("ğŸ‰ INTEGRATION TEST PASSED! The full forward pass works correctly. ğŸ‰")
        print("="*60)

    except Exception as e:
        print("" + "="*60)
        print("âŒ TEST FAILED! An error occurred during the forward pass:")
        import traceback
        traceback.print_exc()
        print("="*60)
