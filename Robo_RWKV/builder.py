# builder.py

import torch
import torch.nn as nn
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
import os
os.environ["RWKV_JIT_ON"] = "1" 
os.environ["RWKV_HEAD_SIZE_A"] = str(64)
os.environ["RWKV_CTXLEN"] = str(256) 
import argparse # ç”¨äºä¸ºæµ‹è¯•åˆ›å»ºé…ç½®å¯¹è±¡

# ====================================================================================
# å…³é”®ä¾èµ–é¡¹å¯¼å…¥
# ç¡®ä¿è¿™äº›æ–‡ä»¶ç›¸å¯¹äºæ‚¨çš„é¡¹ç›®æ ¹ç›®å½•æ˜¯å¯è®¿é—®çš„
# å¦‚æœè¿è¡Œæ—¶å‡ºç° ModuleNotFoundErrorï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´è¿™äº›å¯¼å…¥è·¯å¾„
# ä¾‹å¦‚ï¼Œå»æ‰å‰é¢çš„ '.' (from .hidden_model -> from hidden_model)
# ====================================================================================
from src.hidden_model import VisualRWKV
from policy_heads.models.droid_unet_diffusion import ConditionalUnet1D, SinusoidalPosEmb, Downsample1d, Upsample1d, Conv1dBlock, ConditionalResidualBlock1D
from src.rwkv_tokenizer import TRIE_TOKENIZER
from src.dataset import process_image_tokens_in_conversations, preprocess
from src.utils import Conversation

# å®šä¹‰ä¸€äº›VLMå¯èƒ½éœ€è¦çš„å¸¸é‡
IGNORE_INDEX = -100
DEFAULT_IMAGE_TOKEN = "<image>"

class RWKVDroidPolicy(nn.Module):
    """
    é›†æˆçš„ç­–ç•¥æ¨¡å‹ï¼Œå†…éƒ¨åŒ…å«äº†å®Œæ•´çš„VLMè¾“å…¥é¢„å¤„ç†é€»è¾‘ï¼Œå¹¶ä½¿ç”¨çœŸå®ç»„ä»¶ã€‚
    """
    def __init__(self,
                 # æ–‡ä»¶è·¯å¾„
                 vlm_model_path: str,
                 tokenizer_path: str,
                 # VLM é…ç½®
                 vlm_config_args,
                 # Diffusion Head & Action é…ç½®
                 action_dim: int,
                 state_dim: int,
                 action_horizon: int,
                 down_dims: list = [256, 512, 1024],
                 kernel_size: int = 5,
                 n_groups: int = 8,
                 # Scheduler é…ç½®
                 num_train_timesteps: int = 100,
                 num_inference_steps: int = 10,
                 beta_schedule: str = 'squaredcos_cap_v2',
                 prediction_type: str = 'epsilon'
                ):
        super().__init__()
        
        # ä¿å­˜åŠ¨ä½œ/çŠ¶æ€é…ç½®
        self.action_dim = action_dim
        self.state_dim = state_dim # ä¿å­˜çŠ¶æ€ç»´åº¦ï¼Œæ¨ç†æ—¶éœ€è¦
        self.action_horizon = action_horizon
        self.num_inference_steps = num_inference_steps
        
        # 1. åˆå§‹åŒ– VLM å’Œ Tokenizer (ä½¿ç”¨çœŸå®ç»„ä»¶å’Œè·¯å¾„)
        print("æ­£åœ¨åˆå§‹åŒ– VisualRWKV...")
        self.vlm = VisualRWKV(vlm_config_args)
        print(f"æ­£åœ¨ä» {vlm_model_path} åŠ è½½VLMæƒé‡...")
        self.vlm.load_state_dict(torch.load(vlm_model_path, map_location='cpu'), strict=False)
        self.vlm.eval().bfloat16() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å’Œbf16
        
        print(f"æ­£åœ¨ä» {tokenizer_path} åˆå§‹åŒ– TRIE_TOKENIZER...")
        self.tokenizer = TRIE_TOKENIZER(tokenizer_path)
        
        # 2. å†»ç»“VLM
        print("æ­£åœ¨å†»ç»“ VisualRWKV çš„å‚æ•°...")
        for param in self.vlm.parameters():
            param.requires_grad = False
            
        self.global_cond_dim = vlm_config_args.n_embd
        print(f"VLM å…¨å±€æ¡ä»¶ç»´åº¦å·²ç¡®è®¤ä¸º: {self.global_cond_dim}")

        # 3. åˆå§‹åŒ– Diffusion Head
        print("æ­£åœ¨åˆå§‹åŒ– ConditionalUnet1D Diffusion Head...")
        self.diffusion_head = ConditionalUnet1D(
            input_dim=self.action_dim,
            global_cond_dim=self.global_cond_dim,
            state_dim=state_dim,
            down_dims=down_dims,
            kernel_size=kernel_size,
            n_groups=n_groups
        )
        
        self.diffusion_head.to(dtype=torch.bfloat16)

        # 4. åˆå§‹åŒ–å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDIMScheduler(
            num_train_timesteps=num_train_timesteps,
            beta_schedule=beta_schedule,
            clip_sample=True, # å»ºè®®å¼€å¯ï¼Œé˜²æ­¢åŠ¨ä½œå€¼è¶…å‡ºèŒƒå›´
            set_alpha_to_one=True,
            steps_offset=0,
            prediction_type=prediction_type
        )
        
        # ä¿å­˜é¢„å¤„ç†éœ€è¦çš„å‚æ•°
        self.ctx_len = vlm_config_args.ctx_len
        self.image_position = getattr(vlm_config_args, 'image_position', 'first')

    def _prepare_vlm_input(self, image_input, text_instructions, device):
        """ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œå°è£…äº†ä»demo.pyå­¦åˆ°çš„å®Œæ•´é¢„å¤„ç†é€»è¾‘ã€‚"""
        batch_size = image_input.shape[0]
        all_input_ids = []
        all_labels = []

        for i in range(batch_size):
            instruction = text_instructions[i]
            input_text = DEFAULT_IMAGE_TOKEN + ' ' + instruction

            conv = Conversation(id=f"session_{i}", roles=["human", "gpt"], conversations=[])
            conv.append_message(conv.roles[0], input_text)
            
            conversations_processed = process_image_tokens_in_conversations(
                conv.conversations, image_position=self.image_position
            )
            
            data_dict = preprocess(
                conversations_processed,
                self.tokenizer,
                has_image=True,
                ctx_len=self.ctx_len,
                pad_token_id=0,
                do_pad_to_max_length=False
            )
            all_input_ids.append(data_dict['input_ids'])
            all_labels.append(data_dict['labels'])
        
        max_len = max(len(ids) for ids in all_input_ids)
        
        padded_input_ids = torch.zeros(batch_size, max_len, dtype=torch.long, device=device)
        padded_labels = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=torch.long, device=device)
        
        for i in range(batch_size):
            len_ids = len(all_input_ids[i])
            padded_input_ids[i, :len_ids] = all_input_ids[i]
            padded_labels[i, :len_ids] = all_labels[i]

        samples = {
            "images": image_input.bfloat16(),
            "input_ids": padded_input_ids,
            "labels": padded_labels
        }
        return samples

    def forward(self, image_input, text_instructions, robot_state, ground_truth_actions):
        """è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­"""
        device = image_input.device
        
        with torch.no_grad():
             samples = self._prepare_vlm_input(image_input, text_instructions, device)
             global_condition = self.vlm.get_global_condition(samples)
        
        noise = torch.randn_like(ground_truth_actions)
        timesteps = torch.randint(
            0, self.noise_scheduler.config.num_train_timesteps, 
            (ground_truth_actions.shape[0],), device=device
        ).long()
        noisy_actions = self.noise_scheduler.add_noise(ground_truth_actions, noise, timesteps)
        
        predicted_noise = self.diffusion_head(
            sample=noisy_actions,
            timestep=timesteps,
            global_cond=global_condition,
            states=robot_state
        )
        
        return predicted_noise, noise

    @torch.no_grad()
    def plan_action(self, image_input, text_instruction, robot_state):
        """
        æ¨ç†/è§„åˆ’å‡½æ•°ï¼Œç”¨äºåœ¨å®é™…ç¯å¢ƒä¸­ç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚
        
        Args:
            image_input (torch.Tensor): å•å¸§å›¾åƒ, shape: [1, 1, C, H, W]
            text_instruction (str): å•ä¸ªæ–‡æœ¬æŒ‡ä»¤, e.g. "pick up the red block"
            robot_state (torch.Tensor): å½“å‰æœºå™¨äººçŠ¶æ€, shape: [1, state_dim]
            
        Returns:
            torch.Tensor: è§„åˆ’å‡ºçš„åŠ¨ä½œåºåˆ—, shape: [1, action_horizon, action_dim]
        """
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.eval()
        device = image_input.device
        dtype = next(self.diffusion_head.parameters()).dtype # è·å–diffusion headçš„æ•°æ®ç±»å‹ (bfloat16)

        # 1. ä½¿ç”¨VLMæå–å…¨å±€æ¡ä»¶
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æŠŠå•ä¸ªæŒ‡ä»¤åŒ…è£…æˆä¸€ä¸ªåˆ—è¡¨ï¼Œä»¥åŒ¹é…_prepare_vlm_inputçš„è¾“å…¥è¦æ±‚
        samples = self._prepare_vlm_input(image_input, [text_instruction], device)
        global_condition = self.vlm.get_global_condition(samples)
        
        # 2. å‡†å¤‡å»å™ªè¿‡ç¨‹çš„åˆå§‹è¾“å…¥
        # åˆå§‹åŒ–ä¸€ä¸ªçº¯å™ªå£°å¼ é‡ä½œä¸ºåŠ¨ä½œåºåˆ—çš„èµ·ç‚¹
        noisy_actions = torch.randn(
            (1, self.action_horizon, self.action_dim), 
            device=device, 
            dtype=dtype
        )
        # å°†æœºå™¨äººçŠ¶æ€è½¬æ¢ä¸ºæ­£ç¡®çš„ç±»å‹å’Œè®¾å¤‡
        robot_state = robot_state.to(device=device, dtype=dtype)

        # 3. è®¾ç½®è°ƒåº¦å™¨çš„æ—¶é—´æ­¥
        self.noise_scheduler.set_timesteps(self.num_inference_steps)

        # 4. é€æ­¥å»å™ªå¾ªç¯
        for t in self.noise_scheduler.timesteps:
            # é¢„æµ‹å½“å‰æ—¶é—´æ­¥çš„å™ªå£°
            # æ³¨æ„ï¼štimestepä¹Ÿéœ€è¦æ˜¯ä¸€ä¸ªtensor
            timestep_tensor = t.unsqueeze(0).to(device)
            
            predicted_noise = self.diffusion_head(
                sample=noisy_actions,
                timestep=timestep_tensor,
                global_cond=global_condition,
                states=robot_state
            )
            
            # ä½¿ç”¨è°ƒåº¦å™¨çš„stepæ–¹æ³•ï¼Œä»å½“å‰å¸¦å™ªåŠ¨ä½œä¸­ç§»é™¤é¢„æµ‹çš„å™ªå£°
            # å¾—åˆ°ä¸€ä¸ªæ›´æ¸…æ™°çš„åŠ¨ä½œåºåˆ—
            noisy_actions = self.noise_scheduler.step(
                model_output=predicted_noise,
                timestep=t,
                sample=noisy_actions
            ).prev_sample

        # 5. è¿”å›æœ€ç»ˆå»å™ªåçš„åŠ¨ä½œåºåˆ—
        return noisy_actions

# ====================================================================================
#                           ä¸»æµ‹è¯•ç”¨ä¾‹ (ä½¿ç”¨çœŸå®ç»„ä»¶)
# ===================================================================================
if __name__ == '__main__':
    print("="*60)
    print("RUNNING RWKVDroidPolicy REAL INTEGRATION TEST")
    print("="*60)

    # ##################################################################
    # ### ç”¨æˆ·ï¼šè¯·åœ¨æ­¤å¤„ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„                       ###
    # ##################################################################
    VLM_MODEL_PATH = "/home/bgi/code/VLA/weights/VisualRWKV-v060-1B6-v1.0-20240612.pth"
    TOKENIZER_PATH = "src/rwkv_vocab_v20230424.txt"
    # ##################################################################

    # --- 1. è®¾ç½®æµ‹è¯•å‚æ•° ---
    ACTION_DIM = 7
    STATE_DIM = 7
    ACTION_HORIZON = 16
    BATCH_SIZE = 2
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"Test Configuration:")
    print(f"  - Device: {DEVICE}")
    print(f"  - Batch Size (for training test): {BATCH_SIZE}")
    print(f"  - Action Dim: {ACTION_DIM}")
    print(f"  - State Dim: {STATE_DIM}")
    print(f"  - VLM Model: {VLM_MODEL_PATH}")
    print(f"  - Tokenizer: {TOKENIZER_PATH}")

    # --- 2. åˆ›å»ºVLMæ‰€éœ€çš„é…ç½®å¯¹è±¡ ---
    vlm_config_args = argparse.Namespace(
        n_layer=24, n_embd=2048, ctx_len=256, vocab_size=65536,
        load_model="", vision_tower_name="/home/bgi/code/VLA/weights/CLIP",
        dim_att=2048, dim_ffn=7168, pre_ffn=0, head_size_a=64,
        head_size_divisor=8, dropout=0.0, grad_cp=0, grid_size=-1,
        image_position='first'
    )
    os.environ["RWKV_HEAD_SIZE_A"] = str(vlm_config_args.head_size_a)

    # --- 3. å®ä¾‹åŒ–æˆ‘ä»¬çš„ä¸»ç­–ç•¥æ¨¡å‹ ---
    try:
        policy = RWKVDroidPolicy(
            vlm_model_path=VLM_MODEL_PATH,
            tokenizer_path=TOKENIZER_PATH,
            vlm_config_args=vlm_config_args,
            action_dim=ACTION_DIM,
            state_dim=STATE_DIM,
            action_horizon=ACTION_HORIZON
        ).to(DEVICE)
    except Exception as e:
        print(f"[ERROR] åˆå§‹åŒ– RWKVDroidPolicy æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        exit()
        
    print("--- RWKVDroidPolicy Initialized Successfully ---")

    # ======================== è®­ç»ƒå‰å‘ä¼ æ’­æµ‹è¯• ========================
    print("--- Testing Training Forward Pass ---")
    
    # åˆ›å»ºè®­ç»ƒç”¨çš„æ‰¹å¤„ç†æ•°æ®
    dummy_train_images = torch.randn(BATCH_SIZE, 1, 3, 336, 336, device=DEVICE)
    dummy_train_instructions = [f"pick up object {i}" for i in range(BATCH_SIZE)]
    dummy_train_state = torch.randn(BATCH_SIZE, STATE_DIM, device=DEVICE, dtype=torch.bfloat16)
    dummy_train_actions = torch.randn(BATCH_SIZE, ACTION_HORIZON, ACTION_DIM, device=DEVICE, dtype=torch.bfloat16)
    
    try:
        policy.train()
        predicted_noise, original_noise = policy.forward(
            image_input=dummy_train_images,
            text_instructions=dummy_train_instructions,
            robot_state=dummy_train_state,
            ground_truth_actions=dummy_train_actions
        )
        print("--- TRAINING FORWARD PASS COMPLETED ---")
        print(f"Output predicted_noise shape: {predicted_noise.shape}")
        assert predicted_noise.shape == dummy_train_actions.shape
        print("[SUCCESS] Output shape matches ground truth action shape.")
        
    except Exception as e:
        print("" + "="*60)
        print("âŒ TRAINING TEST FAILED! An error occurred:")
        import traceback
        traceback.print_exc()
        exit()

    print("" + "="*60 + "")

    # ======================== æ¨ç†/è§„åˆ’å‡½æ•°æµ‹è¯• ========================
    print("--- Testing Inference/Planning Function (plan_action) ---")
    
    # åˆ›å»ºæ¨ç†ç”¨çš„å•æ ·æœ¬æ•°æ®
    dummy_inference_image = torch.randn(1, 1, 3, 336, 336, device=DEVICE)
    dummy_inference_instruction = "put the green cup into the sink"
    dummy_inference_state = torch.randn(1, STATE_DIM, device=DEVICE)

    try:
        planned_actions = policy.plan_action(
            image_input=dummy_inference_image,
            text_instruction=dummy_inference_instruction,
            robot_state=dummy_inference_state
        )
        print("--- INFERENCE (plan_action) COMPLETED ---")
        print(f"Output planned_actions shape: {planned_actions.shape}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (1, ACTION_HORIZON, ACTION_DIM)
        assert planned_actions.shape == expected_shape
        print(f"[SUCCESS] Output shape {planned_actions.shape} matches expected shape {expected_shape}.")

        print("" + "="*60)
        print("ğŸ‰ ALL TESTS PASSED! The model works for both training and inference. ğŸ‰")
        print("="*60)

    except Exception as e:
        print("" + "="*60)
        print("âŒ INFERENCE TEST FAILED! An error occurred in plan_action:")
        import traceback
        traceback.print_exc()
        exit()
